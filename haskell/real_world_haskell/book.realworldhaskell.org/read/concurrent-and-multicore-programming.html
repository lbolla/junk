<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><title>Chapter 24. Concurrent and multicore programming</title><link rel="stylesheet" href="/support/styles.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.73.2"><link rel="start" href="index.html" title="Real World Haskell"><link rel="up" href="index.html" title="Real World Haskell"><link rel="prev" href="gui-programming-with-gtk-hs.html" title="Chapter 23. GUI Programming with gtk2hs"><link rel="next" href="profiling-and-optimization.html" title="Chapter 25. Profiling and optimization"><link rel="alternate" type="application/atom+xml" title="Comments" href="/feeds/comments/"><link rel="shortcut icon" type="image/png" href="/support/figs/favicon.png"><script type="text/javascript" src="/support/jquery-min.js"></script><script type="text/javascript" src="/support/form.js"></script><script type="text/javascript" src="/support/hsbook.js"></script></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="navheader"><h2 class="booktitle"><a href="/">Real World Haskell</a><span class="authors">by Bryan O'Sullivan, Don Stewart, and John Goerzen</span></h2></div><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">Chapter 24. Concurrent and multicore programming</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="gui-programming-with-gtk-hs.html">Prev</a> </td><th width="60%" align="center"> </th><td width="20%" align="right"> <a accesskey="n" href="profiling-and-optimization.html">Next</a></td></tr></table></div><div class="chapter" lang="en" id="concurrent"><div class="titlepage"><div><div><h2 class="title">Chapter 24. Concurrent and multicore programming</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="sect1"><a href="concurrent-and-multicore-programming.html#id672744">Defining concurrency and parallelism</a></span></dt><dt><span class="sect1"><a href="concurrent-and-multicore-programming.html#id672818">Concurrent programming with threads</a></span></dt><dd><dl><dt><span class="sect2"><a href="concurrent-and-multicore-programming.html#id672920">Threads are nondeterministic</a></span></dt><dt><span class="sect2"><a href="concurrent-and-multicore-programming.html#id672958">Hiding latency</a></span></dt></dl></dd><dt><span class="sect1"><a href="concurrent-and-multicore-programming.html#id673028">Simple communication between threads</a></span></dt><dt><span class="sect1"><a href="concurrent-and-multicore-programming.html#id673306">The main thread and waiting for other threads</a></span></dt><dd><dl><dt><span class="sect2"><a href="concurrent-and-multicore-programming.html#id673477">Safely modifying an MVar</a></span></dt><dt><span class="sect2"><a href="concurrent-and-multicore-programming.html#id673648">Safe resource management: a good idea, and easy
	besides</a></span></dt><dt><span class="sect2"><a href="concurrent-and-multicore-programming.html#id673762">Finding the status of a thread</a></span></dt><dt><span class="sect2"><a href="concurrent-and-multicore-programming.html#id673978">Writing tighter code</a></span></dt></dl></dd><dt><span class="sect1"><a href="concurrent-and-multicore-programming.html#id674105">Communicating over channels</a></span></dt><dt><span class="sect1"><a href="concurrent-and-multicore-programming.html#id674167">Useful things to know about</a></span></dt><dd><dl><dt><span class="sect2"><a href="concurrent-and-multicore-programming.html#concurrent.useful.nonstrict">MVar and Chan are non-strict</a></span></dt><dt><span class="sect2"><a href="concurrent-and-multicore-programming.html#id674348">Chan is unbounded</a></span></dt></dl></dd><dt><span class="sect1"><a href="concurrent-and-multicore-programming.html#concurrent.hard">Shared-state concurrency is still hard</a></span></dt><dd><dl><dt><span class="sect2"><a href="concurrent-and-multicore-programming.html#id674409">Deadlock</a></span></dt><dt><span class="sect2"><a href="concurrent-and-multicore-programming.html#id674536">Starvation</a></span></dt><dt><span class="sect2"><a href="concurrent-and-multicore-programming.html#id674611">Is there any hope?</a></span></dt></dl></dd><dt><span class="sect1"><a href="concurrent-and-multicore-programming.html#id674634">Exercises</a></span></dt><dt><span class="sect1"><a href="concurrent-and-multicore-programming.html#id674755">Using multiple cores with GHC</a></span></dt><dd><dl><dt><span class="sect2"><a href="concurrent-and-multicore-programming.html#id674846">Runtime options</a></span></dt><dt><span class="sect2"><a href="concurrent-and-multicore-programming.html#id674926">Finding the number of available cores from Haskell</a></span></dt><dt><span class="sect2"><a href="concurrent-and-multicore-programming.html#id675011">Choosing the right runtime</a></span></dt></dl></dd><dt><span class="sect1"><a href="concurrent-and-multicore-programming.html#id675076">Parallel programming in Haskell</a></span></dt><dd><dl><dt><span class="sect2"><a href="concurrent-and-multicore-programming.html#id675112">Normal form and head normal form</a></span></dt><dt><span class="sect2"><a href="concurrent-and-multicore-programming.html#id675157">Sequential sorting</a></span></dt><dt><span class="sect2"><a href="concurrent-and-multicore-programming.html#id675263">Transforming our code into parallel code</a></span></dt><dt><span class="sect2"><a href="concurrent-and-multicore-programming.html#id675459">Knowing what to evaluate in parallel</a></span></dt><dt><span class="sect2"><a href="concurrent-and-multicore-programming.html#id675619">What promises does par make?</a></span></dt><dt><span class="sect2"><a href="concurrent-and-multicore-programming.html#id675679">Running our code, and measuring performance</a></span></dt><dt><span class="sect2"><a href="concurrent-and-multicore-programming.html#id676133">Tuning for performance</a></span></dt><dt><span class="sect2"><a href="concurrent-and-multicore-programming.html#id676322">Exercises</a></span></dt></dl></dd><dt><span class="sect1"><a href="concurrent-and-multicore-programming.html#id676390">Parallel strategies and MapReduce</a></span></dt><dd><dl><dt><span class="sect2"><a href="concurrent-and-multicore-programming.html#concurrent.strategies">Separating algorithm from evaluation</a></span></dt><dt><span class="sect2"><a href="concurrent-and-multicore-programming.html#id676854">Separating algorithm from strategy</a></span></dt><dt><span class="sect2"><a href="concurrent-and-multicore-programming.html#id676990">Writing a simple MapReduce definition</a></span></dt><dt><span class="sect2"><a href="concurrent-and-multicore-programming.html#id677069">MapReduce and strategies</a></span></dt><dt><span class="sect2"><a href="concurrent-and-multicore-programming.html#id677123">Sizing work appropriately</a></span></dt><dd><dl><dt><span class="sect3"><a href="concurrent-and-multicore-programming.html#id677193">Mitigating the risks of lazy I/O</a></span></dt></dl></dd><dt><span class="sect2"><a href="concurrent-and-multicore-programming.html#id677345">Efficiently finding line-aligned chunks</a></span></dt><dt><span class="sect2"><a href="concurrent-and-multicore-programming.html#id677400">Counting lines</a></span></dt><dt><span class="sect2"><a href="concurrent-and-multicore-programming.html#id677449">Finding the most popular URLs</a></span></dt><dt><span class="sect2"><a href="concurrent-and-multicore-programming.html#id677523">Conclusions</a></span></dt></dl></dd></dl></div><p id="x_X31"><a name="x_X31"></a>As we write this book, the landscape of CPU architecture is
    changing more rapidly than it has in decades.  </p><div class="sect1" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both" id="id672744">Defining concurrency and parallelism</h2></div></div></div><p id="x_Y31"><a name="x_Y31"></a>A <span class="emphasis"><em>concurrent</em></span> program needs
      to perform several possibly unrelated tasks at the same time.
      Consider the example of a game server: it is typically composed
      of dozens of components, each of which has complicated
      interactions with the outside world.  One component might handle
      multi-user chat; several more will process the inputs of
      players, and feed state updates back to them; while another
      performs physics calculations.</p><p id="x_Z31"><a name="x_Z31"></a>The correct operation of a concurrent program does not
      require multiple cores, though they may improve performance and
      responsiveness.</p><p id="x_a31"><a name="x_a31"></a>In contrast, a <span class="emphasis"><em>parallel</em></span>
      program solves a single problem.  Consider a financial model
      that attempts to predict the next minute of fluctuations in the
      price of a single stock.  If we want to apply this model to
      every stock listed on an exchange, for example to estimate which
      ones we should buy and sell, we hope to get an answer more
      quickly if we run the model on five hundred cores than if we use
      just one. As this suggests, a parallel program does not usually
      depend on the presence of multiple cores to work
      correctly.</p><p id="x_b31"><a name="x_b31"></a>Another useful distinction between concurrent and parallel
      programs lies in their interaction with the outside world.  By
      definition, a concurrent program deals continuously with
      networking protocols, databases, and the like.  A typical
      parallel program is likely to be more focused: it streams data
      in, crunches it for a while (with little further I/O), then
      streams data back out.</p><p id="x_jJ1"><a name="x_jJ1"></a>Many traditional languages further blur the
      already indistinct boundary between concurrent and parallel
      programming, because they force programmers to use the same
      primitives to construct both kinds of program.</p><p id="x_c31"><a name="x_c31"></a>In this chapter, we will concern ourselves with concurrent
      and parallel programs that operate within the boundaries of a
      single operating system process.</p></div><div class="sect1" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both" id="id672818">Concurrent programming with threads</h2></div></div></div><p id="x_d31"><a name="x_d31"></a>As a building block for concurrent programs, most
      programming languages provide a way of creating multiple
      independent <span class="emphasis"><em>threads of control</em></span>.  Haskell is
      no exception, though programming with threads in Haskell looks
      somewhat different than in other languages.</p><p id="x_e31"><a name="x_e31"></a>In Haskell, a thread is an <span class="type">IO</span> action that
      executes independently from other threads.  To create a thread,
      we import the <code class="code">Control.Concurrent</code> module and use the
      <code class="function">forkIO</code> function.</p><a name="forkIO.ghci:forkIO"></a><pre id="forkIO.ghci:forkIO" class="screen"><code class="prompt">ghci&gt; </code><strong class="userinput"><code>:m +Control.Concurrent</code></strong>
<code class="prompt">ghci&gt; </code><strong class="userinput"><code>:t forkIO</code></strong>
forkIO :: IO () -&gt; IO ThreadId
<code class="prompt">ghci&gt; </code><strong class="userinput"><code>:m +System.Directory</code></strong>
<code class="prompt">ghci&gt; </code><strong class="userinput"><code>forkIO (writeFile "xyzzy" "seo craic nua!") &gt;&gt; doesFileExist "xyzzy"</code></strong>
False
</pre><p id="x_f31"><a name="x_f31"></a>The new thread starts to execute almost immediately, and the
      thread that created it continues to execute concurrently.  The
      thread will stop executing when it reaches the end of its <span class="type">IO</span>
      action.</p><div class="sect2" lang="en"><div class="titlepage"><div><div><h3 class="title" id="id672920">Threads are nondeterministic</h3></div></div></div><p id="x_g31"><a name="x_g31"></a>The runtime component of <span class="application">GHC</span> does not specify an order
	in which it executes threads.  As a result, in our example
	above, the file <code class="filename">xyzzy</code> created by the new
	thread <span class="emphasis"><em>may or may not</em></span> have been created
	by the time the original thread checks for its existence. If
	we try this example once, then remove
	<code class="filename">xyzzy</code> and try again, we may get a
	different result the second time.</p></div><div class="sect2" lang="en"><div class="titlepage"><div><div><h3 class="title" id="id672958">Hiding latency</h3></div></div></div><p id="x_h31"><a name="x_h31"></a>Suppose we have a large file to compress and write to
	disk, but we want to handle a user's input quickly enough that
	they will perceive our program as responding immediately.  If
	we use <code class="function">forkIO</code> to write the file out in a
	separate thread, we can do both simultaneously.</p><a name="Compressor.hs:module"></a><pre id="Compressor.hs:module" class="programlisting">-- file: ch24/Compressor.hs
import Control.Concurrent (forkIO)
import Control.Exception (handle)
import Control.Monad (forever)
import qualified Data.ByteString.Lazy as L
import System.Console.Readline (readline)

-- Provided by the 'zlib' package on http://hackage.haskell.org/
import Codec.Compression.GZip (compress)

main = do
    maybeLine &lt;- readline "Enter a file to compress&gt; "
    case maybeLine of
      Nothing -&gt; return ()      -- user entered EOF
      Just "" -&gt; return ()      -- treat no name as "want to quit"
      Just name -&gt; do
           handle print $ do
             content &lt;- L.readFile name
             forkIO (compressFile name content)
             return ()
           main
  where compressFile path = L.writeFile (path ++ ".gz") . compress</pre><p id="x_i31"><a name="x_i31"></a>Because we're using lazy <span class="type">ByteString</span> I/O here,
	all we really do in the main thread is open the file.  The
	actual reading occurs on demand in the other thread.</p><p id="x_j31"><a name="x_j31"></a>The use of <code class="code">handle print</code> gives us a cheap
	way to print an error message if the user enters the name of a
	file that does not exist.</p></div></div><div class="sect1" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both" id="id673028">Simple communication between threads</h2></div></div></div><p id="x_k31"><a name="x_k31"></a>The simplest way to share information between two threads is
      to let them both use a variable.  In our file compression
      example, the <code class="function">main</code> thread shares both the
      name of a file and its contents with the other thread.  Because
      Haskell data is immutable by default, this poses no risks:
      neither thread can modify the other's view of the file's name or
      contents.</p><p id="x_l31"><a name="x_l31"></a>We often need to have threads actively communicate with each
      other. For example, <span class="application">GHC</span> does not provide a way for one thread
      to find out whether another is still executing, has completed,
      or has crashed<sup>[<a name="id673063" href="#ftn.id673063" class="footnote">54</a>]</sup>.  However, it provides a <span class="emphasis"><em>synchronizing
	variable</em></span> type, the <span class="type">MVar</span>, which we can
      use to create this capability for ourselves.</p><p id="x_n31"><a name="x_n31"></a>An <span class="type">MVar</span> acts like a single-element box: it can
      be either full or empty.  We can put something into the box,
      making it full, or take something out, making it empty.</p><a name="mvar.ghci:MVar"></a><pre id="mvar.ghci:MVar" class="screen"><code class="prompt">ghci&gt; </code><strong class="userinput"><code>:t putMVar</code></strong>
putMVar :: MVar a -&gt; a -&gt; IO ()
<code class="prompt">ghci&gt; </code><strong class="userinput"><code>:t takeMVar</code></strong>
takeMVar :: MVar a -&gt; IO a
</pre><p id="x_o31"><a name="x_o31"></a>If we try to put a value into an <span class="type">MVar</span> that is
      already full, our thread is put to sleep until another thread
      takes the value out.  Similarly, if we try to take a value from
      an empty <span class="type">MVar</span>, our thread is put to sleep until
      some other thread puts a value in.</p><a name="MVarExample.hs:communicate"></a><pre id="MVarExample.hs:communicate" class="programlisting">-- file: ch24/MVarExample.hs
import Control.Concurrent

communicate = do
  m &lt;- newEmptyMVar
  forkIO $ do
    v &lt;- takeMVar m
    putStrLn ("received " ++ show v)
  putStrLn "sending"
  putMVar m "wake up!"</pre><p id="x_p31"><a name="x_p31"></a>The <code class="function">newEmptyMVar</code> function has a
      descriptive name.  To create an <span class="type">MVar</span> that starts
      out non-empty, we'd use <code class="function">newMVar</code>.</p><a name="mvar.ghci:new"></a><pre id="mvar.ghci:new" class="screen"><code class="prompt">ghci&gt; </code><strong class="userinput"><code>:t newEmptyMVar</code></strong>
newEmptyMVar :: IO (MVar a)
<code class="prompt">ghci&gt; </code><strong class="userinput"><code>:t newMVar</code></strong>
newMVar :: a -&gt; IO (MVar a)
</pre><p id="x_q31"><a name="x_q31"></a>Let's run our example in <span class="command"><strong>ghci</strong></span>.</p><a name="mvar.ghci:communicate"></a><pre id="mvar.ghci:communicate" class="screen"><code class="prompt">ghci&gt; </code><strong class="userinput"><code>:load MVarExample</code></strong>
[1 of 1] Compiling Main             ( MVarExample.hs, interpreted )
Ok, modules loaded: Main.
<code class="prompt">ghci&gt; </code><strong class="userinput"><code>communicate</code></strong>
sending
rece</pre><p id="x_r31"><a name="x_r31"></a>If you're coming from a background of concurrent programming
      in a traditional language, you can think of an <span class="type">MVar</span>
      as being useful for two familiar purposes.</p><div class="itemizedlist"><ul type="disc"><li><p id="x_s31"><a name="x_s31"></a>Sending a message from one thread to another, e.g. a
	  notification.</p></li><li><p id="x_t31"><a name="x_t31"></a>Providing <span class="emphasis"><em>mutual exclusion</em></span> for a
	  piece of mutable data that is shared among threads.  We put
	  the data into the <span class="type">MVar</span> when it is not being
	  used by any thread, and one thread takes it out temporarily
	  to read or modify it.</p></li></ul></div></div><div class="sect1" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both" id="id673306">The main thread and waiting for other threads</h2></div></div></div><p id="x_u31"><a name="x_u31"></a><span class="application">GHC</span>'s runtime system treats the program's original thread
      of control differently from other threads.  When this thread
      finishes executing, the runtime system considers the program as
      a whole to have completed.  If any other threads are executing
      at the time, they are terminated.</p><p id="x_v31"><a name="x_v31"></a>As a result, when we have long-running threads that must not
      be killed, we must make special arrangements to ensure that the
      main thread doesn't complete until the others do.  Let's develop
      a small library that makes this easy to do.</p><a name="NiceFork.hs:header"></a><pre id="NiceFork.hs:header" class="programlisting">-- file: ch24/NiceFork.hs
import Control.Concurrent
import Control.Exception (Exception, try)
import qualified Data.Map as M

data ThreadStatus = Running
                  | Finished         -- terminated normally
                  | Threw Exception  -- killed by uncaught exception
                    deriving (Eq, Show)

-- | Create a new thread manager.
newManager :: IO ThreadManager

-- | Create a new managed thread.
forkManaged :: ThreadManager -&gt; IO () -&gt; IO ThreadId

-- | Immediately return the status of a managed thread.
getStatus :: ThreadManager -&gt; ThreadId -&gt; IO (Maybe ThreadStatus)

-- | Block until a specific managed thread terminates.
waitFor :: ThreadManager -&gt; ThreadId -&gt; IO (Maybe ThreadStatus)

-- | Block until all managed threads terminate.
waitAll :: ThreadManager -&gt; IO ()</pre><p id="x_w31"><a name="x_w31"></a>We keep our <span class="type">ThreadManager</span> type abstract using
      the usual recipe: we wrap it in a <code class="code">newtype</code>, and prevent clients
      from creating values of this type.  Among our module's exports,
      we list the type constructor and the <span class="type">IO</span> action that
      constructs a manager, but we do not export the data
      constructor.</p><a name="NiceFork.hs:module"></a><pre id="NiceFork.hs:module" class="programlisting">-- file: ch24/NiceFork.hs
module NiceFork
    (
      ThreadManager
    , newManager
    , forkManaged
    , getStatus
    , waitFor
    , waitAll
    ) where</pre><p id="x_x31"><a name="x_x31"></a>For the implementation of <span class="type">ThreadManager</span>, we
      maintain a map from thread ID to thread state.  We'll refer to
      this as the <span class="emphasis"><em>thread map</em></span>.</p><a name="NiceFork.hs:ThreadManager"></a><pre id="NiceFork.hs:ThreadManager" class="programlisting">-- file: ch24/NiceFork.hs
newtype ThreadManager =
    Mgr (MVar (M.Map ThreadId (MVar ThreadStatus)))
    deriving (Eq)

newManager = Mgr `fmap` newMVar M.empty</pre><p id="x_y31"><a name="x_y31"></a>We have two levels of <span class="type">MVar</span> use here.  We keep
      the <span class="type">Map</span> in an <span class="type">MVar</span>.  This lets us
      “<span class="quote">modify</span>” the map by replacing it with a new
      version.  We also ensure that any thread that uses the
      <span class="type">Map</span> will see a consistent view of it.</p><p id="x_z31"><a name="x_z31"></a>For each thread that we manage, we maintain an
      <span class="type">MVar</span>.  A per-thread <span class="type">MVar</span> starts off
      empty, which indicates that the thread is executing.  When the
      thread finishes or is killed by an uncaught exception, we put
      this information into the <span class="type">MVar</span>.</p><p id="x_A41"><a name="x_A41"></a>To create a thread and watch its status, we must perform a
      little bit of book-keeping.</p><a name="NiceFork.hs:forkManaged"></a><pre id="NiceFork.hs:forkManaged" class="programlisting">-- file: ch24/NiceFork.hs
forkManaged (Mgr mgr) body =
    modifyMVar mgr $ \m -&gt; do
      state &lt;- newEmptyMVar
      tid &lt;- forkIO $ do
        result &lt;- try body
        putMVar state (either Threw (const Finished) result)
      return (M.insert tid state m, tid)</pre><div class="sect2" lang="en"><div class="titlepage"><div><div><h3 class="title" id="id673477">Safely modifying an MVar</h3></div></div></div><p id="x_B41"><a name="x_B41"></a>The <code class="function">modifyMVar</code> function that we used
	in <code class="function">forkManaged</code> above is very useful: it's
	a safe combination of <code class="function">takeMVar</code> and
	<code class="function">putMVar</code>.</p><a name="mvar.ghci:modifyMVar"></a><pre id="mvar.ghci:modifyMVar" class="screen"><code class="prompt">ghci&gt; </code><strong class="userinput"><code>:t modifyMVar</code></strong>
ved "wake up!"
modifyMVar :: MVar a -&gt; (a -&gt; IO (a, b)) -&gt; IO b
</pre><p id="x_C41"><a name="x_C41"></a>It takes the value from an <span class="type">MVar</span>, and passes
	it to a function.  This function can both generate a new value
	and return a result.  If the function throws an exception,
	<code class="function">modifyMVar</code> puts the original value back
	into the <span class="type">MVar</span>, otherwise it puts the new value
	in. It returns the other element of the function as its own
	result.</p><p id="x_D41"><a name="x_D41"></a>When we use <code class="function">modifyMVar</code> instead of
	manually managing an <span class="type">MVar</span> with
	<code class="function">takeMVar</code> and
	<code class="function">putMVar</code>, we avoid two common kinds of
	concurrency bug.</p><div class="itemizedlist"><ul type="disc"><li><p id="x_E41"><a name="x_E41"></a>Forgetting to put a value back into an
	    <span class="type">MVar</span>.  This can result in
	    <span class="emphasis"><em>deadlock</em></span>, in which some thread waits
	    forever on an <span class="type">MVar</span> that will never have a
	    value put into it.</p></li><li><p id="x_F41"><a name="x_F41"></a>Failure to account for the possibility that an
	    exception might be thrown, disrupting the flow of a piece
	    of code. This can result in a call to
	    <code class="function">putMVar</code> that
	    <span class="emphasis"><em>should</em></span> occur not actually happening,
	    again leading to deadlock.</p></li></ul></div><p id="x_G41"><a name="x_G41"></a>Because of these nice safety properties, it's wise to use
	<code class="function">modifyMVar</code> whenever possible.</p></div><div class="sect2" lang="en"><div class="titlepage"><div><div><h3 class="title" id="id673648">Safe resource management: a good idea, and easy
	besides</h3></div></div></div><p id="x_H41"><a name="x_H41"></a>We can the take the pattern that
	<code class="function">modifyMVar</code> follows, and apply it to many
	other resource management situations.  Here are the steps of
	the pattern.</p><div class="orderedlist"><ol type="1"><li><p id="x_I41"><a name="x_I41"></a>Acquire a resource.</p></li><li><p id="x_J41"><a name="x_J41"></a>Pass the resource to a function that will do something
	    with it.</p></li><li><p id="x_K41"><a name="x_K41"></a>Always release the resource, even if the function
	    throws an exception. If that occurs, rethrow the exception
	    so it can be caught by application code.</p></li></ol></div><p id="x_L41"><a name="x_L41"></a>Safety aside, this approach has another benefit: it can
	make our code shorter and easier to follow.  As we can see
	from looking at <code class="function">forkManaged</code> above,
	Haskell's lightweight syntax for anonymous functions makes
	this style of coding visually unobtrusive.</p><p id="x_M41"><a name="x_M41"></a>Here's the definition of <code class="function">modifyMVar</code>,
	so that you can see a specific form of this pattern.</p><a name="ModifyMVar.hs:modifyMVar"></a><pre id="ModifyMVar.hs:modifyMVar" class="programlisting">-- file: ch24/ModifyMVar.hs
import Control.Concurrent (MVar, putMVar, takeMVar)
import Control.Exception (block, catch, throw, unblock)
import Prelude hiding (catch) -- use Control.Exception's version

modifyMVar :: MVar a -&gt; (a -&gt; IO (a,b)) -&gt; IO b
modifyMVar m io = 
  block $ do
    a &lt;- takeMVar m
    (b,r) &lt;- unblock (io a) `catch` \e -&gt;
             putMVar m a &gt;&gt; throw e
    putMVar m b
    return r</pre><p id="x_N41"><a name="x_N41"></a>You should easily be able to adapt this to your particular
	needs, whether you're working with network connections,
	database handles, or data managed by a C library.</p></div><div class="sect2" lang="en"><div class="titlepage"><div><div><h3 class="title" id="id673762">Finding the status of a thread</h3></div></div></div><p id="x_O41"><a name="x_O41"></a>Our <code class="function">getStatus</code> function tells us the
	current state of a thread.  If the thread is no longer managed
	(or was never managed in the first place), it returns
	<code class="code">Nothing</code>.</p><a name="NiceFork.hs:getStatus"></a><pre id="NiceFork.hs:getStatus" class="programlisting">-- file: ch24/NiceFork.hs
getStatus (Mgr mgr) tid =
  modifyMVar mgr $ \m -&gt;
    case M.lookup tid m of
      Nothing -&gt; return (m, Nothing)
      Just st -&gt; tryTakeMVar st &gt;&gt;= \mst -&gt; case mst of
                   Nothing -&gt; return (m, Just Running)
                   Just sth -&gt; return (M.delete tid m, Just sth)</pre><p id="x_P41"><a name="x_P41"></a>If the thread is still running, it returns <code class="code">Just
	  Running</code>. Otherwise, it indicates why the thread
	terminated, <span class="emphasis"><em>and</em></span> stops managing the
	thread.</p><p id="x_Q41"><a name="x_Q41"></a>If the <code class="function">tryTakeMVar</code> function finds
	that the <span class="type">MVar</span> is empty, it returns
	<code class="code">Nothing</code> immediately instead of blocking.</p><a name="mvar.ghci:tryTakeMVar"></a><pre id="mvar.ghci:tryTakeMVar" class="screen"><code class="prompt">ghci&gt; </code><strong class="userinput"><code>:t tryTakeMVar</code></strong>
tryTakeMVar :: MVar a -&gt; IO (Maybe a)
</pre><p id="x_R41"><a name="x_R41"></a>Otherwise, it extracts the value from the
	<span class="type">MVar</span> as usual.</p><p id="x_S41"><a name="x_S41"></a>The <code class="function">waitFor</code> function behaves
	similarly, but instead of returning immediately, it blocks
	until the given thread terminates before returning.</p><a name="NiceFork.hs:waitFor"></a><pre id="NiceFork.hs:waitFor" class="programlisting">-- file: ch24/NiceFork.hs
waitFor (Mgr mgr) tid = do
  maybeDone &lt;- modifyMVar mgr $ \m -&gt;
    return $ case M.updateLookupWithKey (\_ _ -&gt; Nothing) tid m of
      (Nothing, _) -&gt; (m, Nothing)
      (done, m') -&gt; (m', done)
  case maybeDone of
    Nothing -&gt; return Nothing
    Just st -&gt; Just `fmap` takeMVar st</pre><p id="x_T41"><a name="x_T41"></a>It first extracts the <span class="type">MVar</span> that holds the
	thread's state, if it exists.  The <span class="type">Map</span> type's
	<code class="function">updateLookupWithKey</code> function is useful:
	it combines looking up a key with modifying or removing the
	value.</p><a name="niceFork.ghci:updateLookupWithKey"></a><pre id="niceFork.ghci:updateLookupWithKey" class="screen"><code class="prompt">ghci&gt; </code><strong class="userinput"><code>:m +Data.Map</code></strong>
<code class="prompt">ghci&gt; </code><strong class="userinput"><code>:t updateLookupWithKey</code></strong>
updateLookupWithKey :: (Ord k) =&gt;
                       (k -&gt; a -&gt; Maybe a) -&gt; k -&gt; Map k a -&gt; (Maybe a, Map k a)
</pre><p id="x_U41"><a name="x_U41"></a>In this case, we want to always remove the
	<span class="type">MVar</span> holding the thread's state if it is present,
	so that our thread manager will no longer be managing the
	thread.  If there was a value to extract, we take the thread's
	exit status from the <span class="type">MVar</span> and return it.</p><p id="x_V41"><a name="x_V41"></a>Our final useful function simply waits for all currently
	managed threads to complete, and ignores their exit
	statuses.</p><a name="NiceFork.hs:waitAll"></a><pre id="NiceFork.hs:waitAll" class="programlisting">-- file: ch24/NiceFork.hs
waitAll (Mgr mgr) = modifyMVar mgr elems &gt;&gt;= mapM_ takeMVar
    where elems m = return (M.empty, M.elems m)</pre></div><div class="sect2" lang="en"><div class="titlepage"><div><div><h3 class="title" id="id673978">Writing tighter code</h3></div></div></div><p id="x_W41"><a name="x_W41"></a>Our definition of <code class="function">waitFor</code> above is a
	little unsatisfactory, because we're performing more or less
	the same case analysis in two places: inside the function
	called by <code class="function">modifyMVar</code>, and again on its
	return value.</p><p id="x_X41"><a name="x_X41"></a>Sure enough, we can apply a function that we came across
	earlier to eliminate this duplication.  The function in
	question is <code class="function">join</code>, from the
	<code class="code">Control.Monad</code> module.</p><a name="niceFork.ghci:join"></a><pre id="niceFork.ghci:join" class="screen"><code class="prompt">ghci&gt; </code><strong class="userinput"><code>:m +Control.Monad</code></strong>
<code class="prompt">ghci&gt; </code><strong class="userinput"><code>:t join</code></strong>
join :: (Monad m) =&gt; m (m a) -&gt; m a
</pre><p id="x_Y41"><a name="x_Y41"></a>The trick here is to see that we can get rid of the second
	<code class="literal">case</code> expression by having the first one return the
	<span class="type">IO</span> action that we should perform once we return
	from <code class="function">modifyMVar</code>.  We'll use
	<code class="function">join</code> to execute the action.</p><a name="NiceFork.hs:waitFor2"></a><pre id="NiceFork.hs:waitFor2" class="programlisting">-- file: ch24/NiceFork.hs
waitFor2 (Mgr mgr) tid =
  join . modifyMVar mgr $ \m -&gt;
    return $ case M.updateLookupWithKey (\_ _ -&gt; Nothing) tid m of
      (Nothing, _) -&gt; (m, return Nothing)
      (Just st, m') -&gt; (m', Just `fmap` takeMVar st)</pre><p id="x_Z41"><a name="x_Z41"></a>This is an interesting idea: we can create a monadic
	function or action in pure code, then pass it around until we
	end up in a monad where we can use it.  This can be a nimble
	way to write code, once we develop an eye for when it makes
	sense.</p></div></div><div class="sect1" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both" id="id674105">Communicating over channels</h2></div></div></div><p id="x_a41"><a name="x_a41"></a>For one-shot communications between threads, an
      <span class="type">MVar</span> is perfectly good.  Another type,
      <span class="type">Chan</span>, provides a one-way communication channel.
      Here is a simple example of its use.</p><a name="Chan.hs:chanExample"></a><pre id="Chan.hs:chanExample" class="programlisting">-- file: ch24/Chan.hs
import Control.Concurrent
import Control.Concurrent.Chan

chanExample = do
  ch &lt;- newChan
  forkIO $ do
    writeChan ch "hello world"
    writeChan ch "now i quit"
  readChan ch &gt;&gt;= print
  readChan ch &gt;&gt;= print</pre><p id="x_b41"><a name="x_b41"></a>If a <span class="type">Chan</span> is empty,
      <code class="function">readChan</code> blocks until there is a value to
      read.  The <code class="function">writeChan</code> function never blocks:
      it writes a new value into a <span class="type">Chan</span>
      immediately.</p></div><div class="sect1" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both" id="id674167">Useful things to know about</h2></div></div></div><div class="sect2" lang="en"><div class="titlepage"><div><div><h3 class="title" id="concurrent.useful.nonstrict">MVar and Chan are non-strict</h3></div></div></div><p id="x_c41"><a name="x_c41"></a>Like most Haskell container types, both <span class="type">MVar</span>
	and <span class="type">Chan</span> are non-strict: neither evaluates its
	contents.  We mention this not because it's a problem, but
	because it's a common blind spot: people tend to assume that
	these types are strict, perhaps because they're used in the
	<span class="type">IO</span> monad.</p><p id="x_d41"><a name="x_d41"></a>As for other container types, the upshot of a mistaken
	guess about the strictness of an <span class="type">MVar</span> or
	<span class="type">Chan</span> type is often a space or performance leak.
	Here's a plausible scenario to consider.</p><p id="x_e41"><a name="x_e41"></a>We fork off a thread to perform some expensive computation
	on another core.</p><a name="Expensive.hs:notQuiteRight"></a><pre id="Expensive.hs:notQuiteRight" class="programlisting">-- file: ch24/Expensive.hs
import Control.Concurrent

notQuiteRight = do
  mv &lt;- newEmptyMVar
  forkIO $ expensiveComputation_stricter mv
  someOtherActivity
  result &lt;- takeMVar mv
  print result</pre><p id="x_f41"><a name="x_f41"></a>It <span class="emphasis"><em>seems</em></span> to do something, and puts
	its result back into the <span class="type">MVar</span>.</p><a name="Expensive.hs:expensiveComputation"></a><pre id="Expensive.hs:expensiveComputation" class="programlisting">-- file: ch24/Expensive.hs
expensiveComputation mv = do
  let a = "this is "
      b = "not really "
      c = "all that expensive"
  putMVar mv (a ++ b ++ c)</pre><p id="x_g41"><a name="x_g41"></a>When we take the result from the <span class="type">MVar</span> in the
	parent thread and attempt to do something with it, our thread
	starts computing furiously, because we never forced the
	computation to actually occur in the other thread!</p><p id="x_h41"><a name="x_h41"></a>As usual, the solution is straightforward, once we know
	there's a potential for a problem: we add strictness to the
	forked thread, to ensure that the computation occurs there.
	This strictness is best added in one place, to avoid the
	possibility that we might forget to add it.</p><a name="ModifyMVarStrict.hs:modifyMVar_strict"></a><pre id="ModifyMVarStrict.hs:modifyMVar_strict" class="programlisting">-- file: ch24/ModifyMVarStrict.hs
{-# LANGUAGE BangPatterns #-}

import Control.Concurrent (MVar, putMVar, takeMVar)
import Control.Exception (block, catch, throw, unblock)
import Prelude hiding (catch) -- use Control.Exception's version

modifyMVar_strict :: MVar a -&gt; (a -&gt; IO a) -&gt; IO ()
modifyMVar_strict m io = block $ do
  a &lt;- takeMVar m
  !b &lt;- unblock (io a) `catch` \e -&gt;
        putMVar m a &gt;&gt; throw e
  putMVar m b</pre><div class="tip"><table border="0" summary="Tip: It's always worth checking Hackage"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Tip]" src="/support/figs/tip.png"></td><th align="left">It's always worth checking Hackage</th></tr><tr><td align="left" valign="top"><p id="x_i41"><a name="x_i41"></a>In the Hackage package database, you will find a
	  library, <code class="code">strict-concurrency</code>, that provides
	  strict versions of the <span class="type">MVar</span> and
	  <span class="type">Chan</span> types.</p></td></tr></table></div><p id="x_kJ1"><a name="x_kJ1"></a>The <code class="code">!</code> pattern above is simple to use, but it
	is not always sufficient to ensure that our data is
	evaluated.  For a more complete approach, see <a class="xref" href="concurrent-and-multicore-programming.html#concurrent.strategies" title="Separating algorithm from evaluation">the section called “Separating algorithm from evaluation”</a> below.</p></div><div class="sect2" lang="en"><div class="titlepage"><div><div><h3 class="title" id="id674348">Chan is unbounded</h3></div></div></div><p id="x_j41"><a name="x_j41"></a>Because <code class="function">writeChan</code> always succeeds
	immediately, there is a potential risk to using a
	<span class="type">Chan</span>.  If one thread writes to a
	<span class="type">Chan</span> more often than another thread reads from
	it, the <span class="type">Chan</span> will grow in an unchecked manner:
	unread messages will pile up as the reader falls further and
	further behind.</p></div></div><div class="sect1" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both" id="concurrent.hard">Shared-state concurrency is still hard</h2></div></div></div><p id="x_k41"><a name="x_k41"></a>Although Haskell has different primitives for sharing data
      between threads than other languages, it still suffers from the
      same fundamental problem: writing correct concurrent programs is
      fiendishly difficult.  Indeed, several pitfalls of concurrent
      programming in other languages apply equally to Haskell.  Two of
      the better known problems are <span class="emphasis"><em>deadlock</em></span> and
      <span class="emphasis"><em>starvation</em></span>.</p><div class="sect2" lang="en"><div class="titlepage"><div><div><h3 class="title" id="id674409">Deadlock</h3></div></div></div><p id="x_l41"><a name="x_l41"></a>In a <span class="emphasis"><em>deadlock</em></span> situation, two or more
	threads get stuck forever in a clash over access to shared
	resources. One classic way to make a multithreaded program
	deadlock is to forget the order in which we must acquire
	locks.  This kind of bug is so common, it has a name:
	<span class="emphasis"><em>lock order inversion</em></span>. While Haskell
	doesn't provide locks, the <span class="type">MVar</span> type is prone to
	the order inversion problem. Here's a simple example.</p><a name="LockHierarchy.hs:nestedModification"></a><pre id="LockHierarchy.hs:nestedModification" class="programlisting">-- file: ch24/LockHierarchy.hs
import Control.Concurrent

nestedModification outer inner = do
  modifyMVar_ outer $ \x -&gt; do
    yield  -- force this thread to temporarily yield the CPU
    modifyMVar_ inner $ \y -&gt; return (y + 1)
    return (x + 1)
  putStrLn "done"

main = do
  a &lt;- newMVar 1
  b &lt;- newMVar 2
  forkIO $ nestedModification a b
  forkIO $ nestedModification b a</pre><p id="x_m41"><a name="x_m41"></a>If we run this in <span class="command"><strong>ghci</strong></span>, it will usually—but not
	always—print nothing, indicating that both threads have
	gotten stuck.</p><p id="x_n41"><a name="x_n41"></a>The problem with the
	<code class="function">nestedModification</code> function is easy to
	spot.  In the first thread, we take the <span class="type">MVar</span>
	<code class="varname">a</code>, then <code class="varname">b</code>.  In the
	second, we take <code class="varname">b</code>, then
	<code class="varname">a</code>.  If the first thread succeeds in taking
	<code class="varname">a</code> and the second takes
	<code class="varname">b</code>, both threads will block: each tries to
	take an <span class="type">MVar</span> that the other has already emptied,
	so neither can make progress.</p><p id="x_o41"><a name="x_o41"></a>Across languages, the usual way to solve an order
	inversion problem is to always follow a consistent order when
	acquiring resources. Since this approach requires manual
	adherence to a coding convention, it is easy to miss in
	practice.</p><p id="x_p41"><a name="x_p41"></a>To make matters more complicated, these kinds of inversion
	problems can be difficult to spot in real code.  The taking of
	<span class="type">MVar</span>s is often spread across several functions in
	different files, making visual inspection more tricky. Worse,
	these problems are often <span class="emphasis"><em>intermittent</em></span>,
	which makes them tough to even reproduce, never mind isolate
	and fix.</p></div><div class="sect2" lang="en"><div class="titlepage"><div><div><h3 class="title" id="id674536">Starvation</h3></div></div></div><p id="x_q41"><a name="x_q41"></a>Concurrent software is also prone to
	<span class="emphasis"><em>starvation</em></span>, in which one thread
	“<span class="quote">hogs</span>” a shared resource, preventing another from
	using it.  It's easy to imagine how this might occur: one
	thread calls <code class="function">modifyMVar</code> with a body that
	executes for 100 milliseconds, while another calls
	<code class="function">modifyMVar</code> on the same <span class="type">MVar</span>
	with a body that executes for 1 millisecond.  The second
	thread cannot make progress until the first puts a value back
	into the <span class="type">MVar</span>.</p><p id="x_r41"><a name="x_r41"></a>The non-strict nature of the <span class="type">MVar</span> type can
	either exacerbate or cause a starvation problem.  If we put a
	thunk into an <span class="type">MVar</span> that will be expensive to
	evaluate, and take it out of the <span class="type">MVar</span> in a thread
	that otherwise looks like it <span class="emphasis"><em>ought</em></span> to be
	cheap, that thread could suddenly become computationally
	expensive if it has to evaluate the thunk.  This makes the
	advice we gave in <a class="xref" href="concurrent-and-multicore-programming.html#concurrent.useful.nonstrict" title="MVar and Chan are non-strict">the section called “MVar and Chan are non-strict”</a>
	particularly relevant.</p></div><div class="sect2" lang="en"><div class="titlepage"><div><div><h3 class="title" id="id674611">Is there any hope?</h3></div></div></div><p id="x_s41"><a name="x_s41"></a>Fortunately, the APIs for concurrency that we have covered
	here are by no means the end of the story.  A more recent
	addition to Haskell, Software Transactional Memory, is both
	easier and safer to work with.  We will discuss it in chapter
	<a class="xref" href="software-transactional-memory.html" title="Chapter 28. Software transactional memory">Chapter 28, <i>Software transactional memory</i></a>.</p></div></div><div class="sect1" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both" id="id674634">Exercises</h2></div></div></div><div class="qandaset"><table border="0" summary="Q and A Set"><col align="left" width="1%"><tbody><tr class="question"><td align="left" valign="top"><a name="id674644"></a><a name="id674646"></a><p><b>1.</b></p></td><td align="left" valign="top"><p id="x_t41"><a name="x_t41"></a>The <span class="type">Chan</span> type is implemented using
	    <span class="type">MVar</span>s.  Use <span class="type">MVar</span>s to develop a
	    <span class="type">BoundedChan</span> library.</p></td></tr><tr class="question"><td align="left" valign="top"><a name="id674673"></a><a name="id674675"></a><p><b>2.</b></p></td><td align="left" valign="top"><p id="x_u41"><a name="x_u41"></a>Your <code class="function">newBoundedChan</code> function
	    should accept an <span class="type">Int</span> parameter, limiting the
	    number of unread items that can be present in a
	    <span class="type">BoundedChan</span> at once.</p></td></tr><tr class="question"><td align="left" valign="top"><a name="id674700"></a><a name="id674702"></a><p><b>3.</b></p></td><td align="left" valign="top"><p id="x_v41"><a name="x_v41"></a>If this limit is hit, a call to your
	    <code class="function">writeBoundedChan</code> function must block
	    until a reader uses <code class="function">readBoundedChan</code>
	    to consume a value.</p></td></tr><tr class="question"><td align="left" valign="top"><a name="id674726"></a><a name="id674728"></a><p><b>4.</b></p></td><td align="left" valign="top"><p id="x_w41"><a name="x_w41"></a>Although we've already mentioned the existence of the
	    <code class="code">strict-concurrency</code> package in the Hackage
	    repository, try developing your own, as a wrapper around
	    the built-in <span class="type">MVar</span> type.  Following classic
	    Haskell practice, make your library type safe, so that
	    users cannot accidentally mix uses of strict and
	    non-strict <span class="type">MVar</span>s.</p></td></tr></tbody></table></div></div><div class="sect1" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both" id="id674755">Using multiple cores with GHC</h2></div></div></div><p id="x_x41"><a name="x_x41"></a>By default, <span class="application">GHC</span> generates programs that use just one core,
      even when we write explicitly concurrent code.  To use multiple
      cores, we must explicitly choose to do so.  We make this choice
      at <span class="emphasis"><em>link time</em></span>, when we are generating an
      executable program.</p><div class="itemizedlist"><ul type="disc"><li><p id="x_y41"><a name="x_y41"></a>The “<span class="quote">non-threaded</span>” runtime library runs all
	  Haskell threads in a single operating system thread.  This
	  runtime is highly efficient for creating threads and passing
	  data around in <span class="type">MVar</span>s.</p></li><li><p id="x_z41"><a name="x_z41"></a>The “<span class="quote">threaded</span>” runtime library uses
	  multiple operating system threads to run Haskell threads.
	  It has somewhat more overhead for creating threads and using
	  <span class="type">MVar</span>s.</p></li></ul></div><p id="x_A51"><a name="x_A51"></a>If we pass the <code class="option">-threaded</code> option to the
      compiler, it will link our program against the threaded runtime
      library.  We do not need to use <code class="option">-threaded</code> when
      we are compiling libraries or source files, only when we are
      finally generating an executable.</p><p id="x_B51"><a name="x_B51"></a>Even when we select the threaded runtime for our program,
      it will still default to using only one core when we run it.  We
      must explicitly tell the runtime how many cores to use.</p><div class="sect2" lang="en"><div class="titlepage"><div><div><h3 class="title" id="id674846">Runtime options</h3></div></div></div><p id="x_C51"><a name="x_C51"></a>We can pass options to <span class="application">GHC</span>'s runtime system on the
	command line of our program.  Before handing control to our
	code, the runtime scans the program's arguments for the
	special command line option <code class="option">+RTS</code>.  It
	interprets everything that follows, until the special option
	<code class="option">-RTS</code>, as an option for the runtime system,
	not our program.  It hides all of these options from our code.
	When we use the <code class="code">System.Environment</code> module's
	<code class="function">getArgs</code>  function to obtain our command
	line arguments, we will not find any runtime options in the
	list.</p><p id="x_D51"><a name="x_D51"></a>The threaded runtime accepts an option
	<code class="option">-N</code><sup>[<a name="id674894" href="#ftn.id674894" class="footnote">55</a>]</sup>.  This takes one argument, which specifies the
	number of cores that <span class="application">GHC</span>'s runtime system should use.  The
	option parser is picky: there must be no spaces between
	<code class="option">-N</code> and the number that follows it.  The option
	<code class="option">-N4</code> is acceptable, but <code class="option">-N 4</code>
	is not.</p></div><div class="sect2" lang="en"><div class="titlepage"><div><div><h3 class="title" id="id674926">Finding the number of available cores from Haskell</h3></div></div></div><p id="x_F51"><a name="x_F51"></a>The module <code class="code">GHC.Conc</code> exports a variable,
	<code class="varname">numCapabilities</code>, that tells us how many
	cores the runtime system has been given with the
	<code class="option">-N</code> RTS option.</p><a name="NumCapabilities.hs:main"></a><pre id="NumCapabilities.hs:main" class="programlisting">-- file: ch24/NumCapabilities.hs
import GHC.Conc (numCapabilities)
import System.Environment (getArgs)

main = do
  args &lt;- getArgs
  putStrLn $ "command line arguments: " ++ show args
  putStrLn $ "number of cores: " ++ show numCapabilities</pre><p id="x_G51"><a name="x_G51"></a>If we compile and run the above program, we can see that
	the options to the runtime system are not visible to the
	program, but that it can see how many cores it can run
	on.</p><pre id="id674971" class="screen"><code class="prompt">$</code> <strong class="userinput"><code>ghc -c NumCapabilities.hs</code></strong>
<code class="prompt">$</code> <strong class="userinput"><code>ghc -threaded -o NumCapabilities NumCapabilities.o</code></strong>
<code class="prompt">$</code> <strong class="userinput"><code>./NumCapabilities +RTS -N4 -RTS foo</code></strong>
command line arguments: ["foo"]
number of cores: 4</pre></div><div class="sect2" lang="en"><div class="titlepage"><div><div><h3 class="title" id="id675011">Choosing the right runtime</h3></div></div></div><p id="x_H51"><a name="x_H51"></a>The decision of which runtime to use is not completely
	clear cut.  While the threaded runtime can use multiple cores,
	it has a cost: threads and sharing data between them are more
	expensive than with the non-threaded runtime.</p><p id="x_I51"><a name="x_I51"></a>Furthermore, the garbage collector used by <span class="application">GHC</span> as of
	version 6.8.3 is single threaded: it pauses all other threads
	while it runs, and executes on one core.  This limits the
	performance improvement we can hope to see from using multiple
	cores<sup>[<a name="id675039" href="#ftn.id675039" class="footnote">56</a>]</sup>.</p><p id="x_J51"><a name="x_J51"></a>In many real world concurrent programs, an individual
	thread will spend most of its time waiting for a network
	request or response. In these cases, if a single Haskell
	program serves tens of thousands of concurrent clients, the
	lower overhead of the non-threaded runtime may be helpful. For
	example, instead of having a single server program use the
	threaded runtime on four cores, we might see better
	performance if we design our server so that we can run four
	copies of it simultaneously, and use the non-threaded
	runtime.</p><p id="x_K51"><a name="x_K51"></a>Our purpose here is not to dissuade you from
	using the threaded runtime.  It is not much more expensive
	than the non-threaded runtime: threads remain amazingly cheap
	compared to the runtimes of most other programming languages.
	We merely want to make it clear that switching to the threaded
	runtime will not necessarily result in an automatic
	win.</p></div></div><div class="sect1" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both" id="id675076">Parallel programming in Haskell</h2></div></div></div><p id="x_L51"><a name="x_L51"></a>We will now switch our focus to parallel programming. For
      many computationally expensive problems, we could calculate a
      result more quickly if we could divide up the solution, and
      evaluate it on many cores at once. Computers with multiple cores
      are already ubiquitous, but few programs can take advantage of
      the computing power of even a modern laptop.</p><p id="x_M51"><a name="x_M51"></a>In large part, this is because parallel programming is
      traditionally seen as very difficult.  In a typical programming
      language, we would use the same libraries and constructs that we
      apply to concurrent programs to develop a parallel program. This
      forces us to contend with the familiar problems of deadlocks,
      race conditions, starvation, and sheer complexity.</p><p id="x_N51"><a name="x_N51"></a>While we could certainly use Haskell's concurrency features
      to develop parallel code, there is a much simpler approach
      available to us.  We can take a normal Haskell function, apply a
      few simple transformations to it, and have it evaluated in
      parallel.</p><div class="sect2" lang="en"><div class="titlepage"><div><div><h3 class="title" id="id675112">Normal form and head normal form</h3></div></div></div><p id="x_mJ1"><a name="x_mJ1"></a>The familiar <code class="function">seq</code> function evaluates
	an expression to what we call <span class="emphasis"><em>head normal
	  form</em></span> (abbreviated HNF).  It stops once it reaches
	the outermost constructor (the “<span class="quote">head</span>”). This is
	distinct from <span class="emphasis"><em>normal form</em></span> (NF), in which
	an expression is completely evaluated.</p><p id="x_nJ1"><a name="x_nJ1"></a>You will also hear Haskell programmers refer to
	<span class="emphasis"><em>weak</em></span> head normal form (WHNF).  For normal
	data, weak head normal form is the same as head normal form.
	The difference only arises for functions, and is too abstruse
	to concern us here.</p></div><div class="sect2" lang="en"><div class="titlepage"><div><div><h3 class="title" id="id675157">Sequential sorting</h3></div></div></div><p id="x_O51"><a name="x_O51"></a>Here is a normal Haskell function that sorts a list using
	a divide-and-conquer approach.</p><a name="Sorting.hs:sort"></a><pre id="Sorting.hs:sort" class="programlisting">-- file: ch24/Sorting.hs
sort :: (Ord a) =&gt; [a] -&gt; [a]
sort (x:xs) = lesser ++ x:greater
    where lesser  = sort [y | y &lt;- xs, y &lt;  x]
          greater = sort [y | y &lt;- xs, y &gt;= x]
sort _ = []</pre><p id="x_P51"><a name="x_P51"></a>This function is inspired by the well-known Quicksort
	algorithm, and it is a classic among Haskell programmers: it
	is often presented as a one-liner early in a Haskell tutorial,
	to tease the reader with an example of Haskell's
	expressiveness. Here, we've split the code over a few lines,
	to make it easier to compare the serial and parallel
	versions.</p><p id="x_Q51"><a name="x_Q51"></a>Here is a very brief description of how
	<code class="function">sort</code> operates.</p><div class="orderedlist"><ol type="1"><li><p id="x_R51"><a name="x_R51"></a>It chooses an element from the list.  This is called
	    the <span class="emphasis"><em>pivot</em></span>.  Any element would do as
	    the pivot; the first is merely the easiest to pattern
	    match on.</p></li><li><p id="x_S51"><a name="x_S51"></a>It creates a sublist of all elements less than the
	    pivot, and recursively sorts them.</p></li><li><p id="x_T51"><a name="x_T51"></a>It creates a sublist of all elements greater than or
	    equal to the pivot, and recursively sorts them.</p></li><li><p id="x_U51"><a name="x_U51"></a>It appends the two sorted sublists.</p></li></ol></div></div><div class="sect2" lang="en"><div class="titlepage"><div><div><h3 class="title" id="id675263">Transforming our code into parallel code</h3></div></div></div><p id="x_V51"><a name="x_V51"></a>The parallel version of the function is only a little more
	complicated than the initial version.</p><a name="Sorting.hs:parSort"></a><pre id="Sorting.hs:parSort" class="programlisting">-- file: ch24/Sorting.hs
module Sorting where

import Control.Parallel (par, pseq)

parSort :: (Ord a) =&gt; [a] -&gt; [a]
parSort (x:xs)    = force greater `par` (force lesser `pseq`
                                         (lesser ++ x:greater))
    where lesser  = parSort [y | y &lt;- xs, y &lt;  x]
          greater = parSort [y | y &lt;- xs, y &gt;= x]
parSort _         = []</pre><p id="x_W51"><a name="x_W51"></a>We have barely perturbed the code: all we have added are
	three functions, <code class="function">par</code>,
	<code class="function">pseq</code>, and
	<code class="function">force</code>.</p><p id="x_X51"><a name="x_X51"></a>The <code class="function">par</code> function is
	provided by the <code class="code">Control.Parallel</code> module.  It
	serves a similar purpose to <code class="function">seq</code>: it
	evaluates its left argument to weak head normal form, and
	returns its right. As its name suggests,
	<code class="function">par</code> can evaluate its left argument in
	parallel with whatever other evaluations are occurring.</p><p id="x_Y51"><a name="x_Y51"></a>As for <code class="function">pseq</code>, it is similar
	to <code class="function">seq</code>: it evaluates the expression on
	the left to WHNF before returning the expression on the right.
	The difference between the two is subtle, but important for
	parallel programs: the compiler does not
	<span class="emphasis"><em>promise</em></span> to evaluate the left argument of
	<code class="function">seq</code> if it can see that evaluating the
	right argument first would improve performance.  This
	flexibility is fine for a program executing on one core, but
	it is not strong enough for code running on multiple cores.
	In contrast, the compiler <span class="emphasis"><em>guarantees</em></span> that
	<code class="function">pseq</code> will evaluate its left argument
	before its right.</p><p id="x_Z51"><a name="x_Z51"></a>These changes to our code are remarkable for all the
	things we have <span class="emphasis"><em>not</em></span> needed to say.</p><div class="itemizedlist"><ul type="disc"><li><p id="x_a51"><a name="x_a51"></a>How many cores to use.</p></li><li><p id="x_b51"><a name="x_b51"></a>What threads do to communicate with each other.</p></li><li><p id="x_c51"><a name="x_c51"></a>How to divide up work among the available cores.</p></li><li><p id="x_d51"><a name="x_d51"></a>Which data are shared between threads, and which are
	    private.</p></li><li><p id="x_e51"><a name="x_e51"></a>How to determine when all the participants are
	    finished.</p></li></ul></div></div><div class="sect2" lang="en"><div class="titlepage"><div><div><h3 class="title" id="id675459">Knowing what to evaluate in parallel</h3></div></div></div><p id="x_f51"><a name="x_f51"></a>The key to getting decent performance out of parallel
	Haskell code is to find meaningful chunks of work to perform
	in parallel.  Non-strict evaluation can get in the way of
	this, which is why we use the <code class="function">force</code>
	function in our parallel sort.  To best explain what the
	<code class="function">force</code> function is for, we will first look
	at a mistaken example.</p><a name="Sorting.hs:sillySort"></a><pre id="Sorting.hs:sillySort" class="programlisting">-- file: ch24/Sorting.hs
sillySort (x:xs) = greater `par` (lesser `pseq`
                                  (lesser ++ x:greater))
    where lesser   = sillySort [y | y &lt;- xs, y &lt;  x]
          greater  = sillySort [y | y &lt;- xs, y &gt;= x]
sillySort _        = []</pre><p id="x_g51"><a name="x_g51"></a>Take a look at the small changes in each use of
	<code class="function">par</code>.  Instead of <code class="code">force
	  lesser</code> and <code class="code">force greater</code>, here we
	evaluate <code class="code">lesser</code> and <code class="code">greater</code>.</p><p id="x_h51"><a name="x_h51"></a>Remember that evaluation to WHNF only computes enough of
	an expression to see its <span class="emphasis"><em>outermost</em></span>
	constructor.  In this mistaken example, we evaluate each
	sorted sublist to WHNF.  Since the outermost constructor in
	each case is just a single list constructor, we are in fact
	only forcing the evaluation of the first element of each
	sorted sublist!  Every other element of each list remains
	unevaluated.  In other words, we do almost no useful work in
	parallel: our <code class="function">sillySort</code> is nearly
	completely sequential.</p><p id="x_i51"><a name="x_i51"></a>We avoid this with our <code class="function">force</code> function
	by forcing the entire spine of a list to be evaluated before
	we give back a constructor.</p><a name="Sorting.hs:force"></a><pre id="Sorting.hs:force" class="programlisting">-- file: ch24/Sorting.hs
force :: [a] -&gt; ()
force xs = go xs `pseq` ()
    where go (_:xs) = go xs
          go [] = 1</pre><p id="x_j51"><a name="x_j51"></a>Notice that we don't care what's in the list;
	  we walk down its spine to the end, then use
	  <code class="function">pseq</code> once.  There is clearly no magic
	  involved here: we are just using our usual understanding of
	  Haskell's evaluation model.  And because we will be using
	  <code class="function">force</code> on the left hand side of
	  <code class="function">par</code> or <code class="function">pseq</code>, we
	  don't need to return a meaningful value.</p><p id="x_oJ1"><a name="x_oJ1"></a>Of course, in many cases we will need to force the
	  evaluation of individual elements of the list, too.  Below,
	  we will discuess a typeclass-based solution to this
	  problem.</p></div><div class="sect2" lang="en"><div class="titlepage"><div><div><h3 class="title" id="id675619">What promises does par make?</h3></div></div></div><p id="x_k51"><a name="x_k51"></a>The <code class="function">par</code> function does not actually
	promise to evaluate an expression in parallel with another.
	Instead, it undertakes to do so if it “<span class="quote">makes
	  sense</span>”.  This wishy-washy non-promise is actually
	more useful than a guarantee to always evaluate an expression
	in parallel.  It gives the runtime system the freedom to act
	intelligently when it encounters a use of
	<code class="function">par</code>.</p><p id="x_l51"><a name="x_l51"></a>For instance, the runtime could decide that an expression
	is too cheap to be worth evaluating in parallel.  Or it might
	notice that all cores are currently busy, so that
	“<span class="quote">sparking</span>” a new parallel evaluation will lead to
	there being more runnable threads than there are cores
	available to execute them.</p><p id="x_m51"><a name="x_m51"></a>This lax specification in turn affects how we write parallel
	code.  Since <code class="function">par</code> may be somewhat
	intelligent at runtime, we can use it almost wherever we like,
	on the assumption that performance will not be bogged down by
	threads contending for busy cores.</p></div><div class="sect2" lang="en"><div class="titlepage"><div><div><h3 class="title" id="id675679">Running our code, and measuring performance</h3></div></div></div><p id="x_n51"><a name="x_n51"></a>To try our code out, let's save <code class="function">sort</code>,
	<code class="function">parSort</code>, and
	<code class="function">parSort2</code> to a module named
	<code class="filename">Sorting.hs</code>.  We create a small driver
	program that we can use to time the performance of one of
	those sorting function.</p><a name="SortMain.hs:main"></a><pre id="SortMain.hs:main" class="programlisting">-- file: ch24/SortMain.hs

module Main where

import Data.Time.Clock (diffUTCTime, getCurrentTime)
import System.Environment (getArgs)
import System.Random (StdGen, getStdGen, randoms)

import Sorting

-- testFunction = sort
-- testFunction = seqSort
testFunction = parSort
-- testFunction = parSort2 2

randomInts :: Int -&gt; StdGen -&gt; [Int]
randomInts k g = let result = take k (randoms g)
                 in force result `seq` result

main = do
  args &lt;- getArgs
  let count | null args = 500000
            | otherwise = read (head args)
  input &lt;- randomInts count `fmap` getStdGen
  putStrLn $ "We have " ++ show (length input) ++ " elements to sort."
  start &lt;- getCurrentTime
  let sorted = testFunction input
  putStrLn $ "Sorted all " ++ show (length sorted) ++ " elements."
  end &lt;- getCurrentTime
  putStrLn $ show (end `diffUTCTime` start) ++ " elapsed."</pre><p id="x_o51"><a name="x_o51"></a>For simplicity, we choose the sorting function to
	benchmark at compilation time, via the
	<code class="varname">testFunction</code> variable.</p><p id="x_p51"><a name="x_p51"></a>Our program accepts a single optional command line
	argument, the length of the random list to generate.</p><p id="x_q51"><a name="x_q51"></a>Non-strict evaluation can turn performance measurement and
	analysis into something of a minefield.  Here are some
	potential problems that we specifically work to avoid in our
	driver program.</p><div class="itemizedlist"><ul type="disc"><li><p id="x_r51"><a name="x_r51"></a><span class="emphasis"><em>Measuring several things, when we think we
	      are looking at just one.</em></span>  Haskell's default
	    pseudorandom number generator (PRNG) is slow, and the
	    <code class="function">randoms</code> function generates random
	    numbers on demand.</p><p id="x_s51"><a name="x_s51"></a>Before we record our starting time, we force every
	    element of the input list to be evaluated, and we print
	    the length of the list: this ensures that we create all of
	    the random numbers that we will need in advance.</p><p id="x_t51"><a name="x_t51"></a>If we were to omit this step, we would interleave the
	    generation of random numbers with attempts to work with
	    them in parallel.  We would thus be measuring both the
	    cost of sorting the numbers and, less obviously, the cost
	    of generating them.</p></li><li><p id="x_u51"><a name="x_u51"></a><span class="emphasis"><em>Invisible data dependencies.</em></span> When
	    we generate the list of random numbers, simply printing
	    the length of the list would not perform enough
	    evaluation. This wouls evaluate the
	    <span class="emphasis"><em>spine</em></span> of the list, but not its
	    elements.  The actual random numbers would not be
	    evaluated until the sort compares them.</p><p id="x_v51"><a name="x_v51"></a>This can have serious consequences for performance.
	    The value of a random number depends on the value of the
	    preceding random number in the list, but we have scattered
	    the list elements randomly among our processor cores.  If
	    we did not evaluate the list elements prior to sorting, we
	    would suffer a terrible “<span class="quote">ping pong</span>” effect:
	    not only would evaluation bounce from one core to another,
	    performance would suffer.</p><p id="x_w51"><a name="x_w51"></a>Try snipping out the application of
	    <code class="function">force</code> from the body of
	    <code class="function">main</code> above: you should find that the
	    parallel code can easily end up three times
	    <span class="emphasis"><em>slower</em></span> than the non-parallel
	    code.</p></li><li><p id="x_x51"><a name="x_x51"></a><span class="emphasis"><em>Benchmarking a thunk, when we believe that
	      the code is performing meaningful work.</em></span>  To
	    force the sort to take place, we print the length of the
	    result list before we record the ending time. Without
	    <code class="function">putStrLn</code> demanding the length of the
	    list in order to print it, the sort would not occur at
	    all.</p></li></ul></div><p id="x_y51"><a name="x_y51"></a>When we build the program, we enable optimization and
	<span class="application">GHC</span>'s threaded runtime.</p><pre id="id675898" class="screen"><code class="prompt">$</code> <strong class="userinput"><code>ghc -threaded -O2 --make SortMain</code></strong>
[1 of 2] Compiling Sorting          ( Sorting.hs, Sorting.o )
[2 of 2] Compiling Main             ( SortMain.hs, SortMain.o )
Linking SortMain ...
</pre><p id="x_z51"><a name="x_z51"></a>When we run the program, we must tell <span class="application">GHC</span>'s
	runtime how many cores to use.  Initially, we try the original
	<code class="function">sort</code>, to establish a performance
	baseline.</p><pre id="id675935" class="screen"><code class="prompt">$</code> <strong class="userinput"><code>./Sorting +RTS -N1 -RTS 700000</code></strong>
We have 700000 elements to sort.
Sorted all 700000 elements.
3.178941s elapsed.
</pre><p id="x_A61"><a name="x_A61"></a>Enabling a second core ought to have no effect on
	performance.</p><pre id="id675960" class="screen"><code class="prompt">$</code> <strong class="userinput"><code>./Sorting +RTS -N2 -RTS 700000</code></strong>
We have 700000 elements to sort.
Sorted all 700000 elements.
3.259869s elapsed.
</pre><p id="x_B61"><a name="x_B61"></a>If we recompile and test the performance of
	<code class="function">parSort</code>, the results are less than
	stellar.</p><pre id="id675990" class="screen"><code class="prompt">$</code> <strong class="userinput"><code>./Sorting +RTS -N1 -RTS 700000</code></strong>
We have 700000 elements to sort.
Sorted all 700000 elements.
3.915818s elapsed.
<code class="prompt">$</code> <strong class="userinput"><code>./Sorting +RTS -N2 -RTS 700000</code></strong>
We have 700000 elements to sort.
Sorted all 700000 elements.
4.029781s elapsed.
</pre><p id="x_C61"><a name="x_C61"></a>We have gained nothing in performance.  It seems that this
	could be due to one of two factors: either
	<code class="function">par</code> is intrinsically expensive, or we are
	using it too much. To help us to distinguish between the two
	possibilities, here is a sort is identical to
	<code class="function">parSort</code>, but it uses
	<code class="function">pseq</code> instead of
	<code class="function">par</code>.</p><a name="Sorting.hs:seqSort"></a><pre id="Sorting.hs:seqSort" class="programlisting">-- file: ch24/Sorting.hs
seqSort :: (Ord a) =&gt; [a] -&gt; [a]
seqSort (x:xs) = lesser `pseq` (greater `pseq`
                                (lesser ++ x:greater))
    where lesser  = seqSort [y | y &lt;- xs, y &lt;  x]
          greater = seqSort [y | y &lt;- xs, y &gt;= x]
seqSort _ = []</pre><p id="x_D61"><a name="x_D61"></a>We also drop the use of <code class="function">force</code>, so
	compared to our original <code class="function">sort</code>, we should
	only be measuring the cost of using <code class="function">pseq</code>.
	What effect does <code class="function">pseq</code> alone have on
	performance?</p><pre id="id676096" class="screen"><code class="prompt">$</code> <strong class="userinput"><code>./Sorting +RTS -N1 -RTS 700000</code></strong>
We have 700000 elements to sort.
Sorted all 700000 elements.
3.848295s elapsed.
</pre><p id="x_E61"><a name="x_E61"></a>This suggests that <code class="function">par</code> and
	<code class="function">pseq</code> have similar costs.  What can we do
	to improve performance?</p></div><div class="sect2" lang="en"><div class="titlepage"><div><div><h3 class="title" id="id676133">Tuning for performance</h3></div></div></div><p id="x_F61"><a name="x_F61"></a>In our <code class="function">parSort</code>, we perform twice as
	many applications of <code class="function">par</code> as there are
	elements to sort. While <code class="function">par</code> is
	<span class="emphasis"><em>cheap</em></span>, as we have seen, it is not
	<span class="emphasis"><em>free</em></span>.  When we recursively apply
	<code class="function">parSort</code>, we eventually apply
	<code class="function">par</code> to individual list elements.  At this
	fine granularity, the cost of using <code class="function">par</code>
	outweighs any possible usefulness.  To reduce this effect, we
	switch to our non-parallel <code class="function">sort</code> after
	passing some threshold.</p><a name="Sorting.hs:parSort2"></a><pre id="Sorting.hs:parSort2" class="programlisting">-- file: ch24/Sorting.hs
parSort2 :: (Ord a) =&gt; Int -&gt; [a] -&gt; [a]
parSort2 d list@(x:xs)
  | d &lt;= 0     = sort list
  | otherwise = force greater `par` (force lesser `pseq`
                                     (lesser ++ x:greater))
      where lesser      = parSort2 d' [y | y &lt;- xs, y &lt;  x]
            greater     = parSort2 d' [y | y &lt;- xs, y &gt;= x]
            d' = d - 1
parSort2 _ _              = []</pre><p id="x_G61"><a name="x_G61"></a>Here, we stop recursing and sparking new parallel
	evaluations at a controllable depth.  If we knew the size of
	the data we were dealing with, we could stop subdividing and
	switch to the non-parallel code once we reached a sufficiently
	small amount of remaining work.</p><pre id="id676222" class="screen"><code class="prompt">$</code> <strong class="userinput"><code>./Sorting +RTS -N2 -RTS 700000</code></strong>
We have 700000 elements to sort.
Sorted all 700000 elements.
2.947872s elapsed.
</pre><p id="x_H61"><a name="x_H61"></a>On a dual core system, this gives us roughly a 25%
	speedup.  This is not a huge number, but consider the number
	of changes we had to make in return for this performance
	improvement: just a few annotations.</p><p id="x_I61"><a name="x_I61"></a>This sorting function is particularly resistant to good
	parallel performance.  The amount of memory allocation it
	performs forces the garbage collector to run frequently.  We
	can see the effect by running our program with the
	<code class="option">-sstderr</code> RTS option, which prints garbage
	collection statistics to the screen.  This indicates that our
	program spends roughly 40% of its time collecting garbage.
	Since the garbage collector in <span class="application">GHC</span> 6.8 stops all threads and
	runs on a single core, it acts as a bottleneck.</p><p id="x_J61"><a name="x_J61"></a>You can expect more impressive performance improvements
	from less allocation-heavy code when you use
	<code class="function">par</code> annotations.  We have seen some
	simple numerical benchmarks run 1.8 times faster on a dual
	core system than with a single core.  As we write this book, a
	parallel garbage collector is under development for <span class="application">GHC</span>,
	which should help considerably with the performance of
	allocation-heavy code on multicore systems.</p><div class="warning"><table border="0" summary="Warning: Beware a GC bug in GHC 6.8.2"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Warning]" src="/support/figs/warning.png"></td><th align="left">Beware a GC bug in GHC 6.8.2</th></tr><tr><td align="left" valign="top"><p id="x_K61"><a name="x_K61"></a>The garbage collector in release 6.8.2 of
	    <span class="application">GHC</span> has a bug that can cause programs using
	    <code class="function">par</code> to crash.  If you want to use
	    <code class="function">par</code> and you are using 6.8.2, we
	    suggest upgrading to at least 6.8.3.</p></td></tr></table></div></div><div class="sect2" lang="en"><div class="titlepage"><div><div><h3 class="title" id="id676322">Exercises</h3></div></div></div><div class="qandaset"><table border="0" summary="Q and A Set"><col align="left" width="1%"><tbody><tr class="question"><td align="left" valign="top"><a name="id676332"></a><a name="id676335"></a><p><b>1.</b></p></td><td align="left" valign="top"><p id="x_L61"><a name="x_L61"></a>It can be difficult to determine when to switch from
	      <code class="function">parSort2</code> to
	      <code class="function">sort</code>.  An alternative approach to
	      the one we outline above would be to decide based on the
	      length of a sublist.  Rewrite
	      <code class="function">parList2</code> so that it
	      switches to <code class="function">sort</code> if the list
	      contains more than some number of elements.</p></td></tr><tr class="question"><td align="left" valign="top"><a name="id676372"></a><a name="id676374"></a><p><b>2.</b></p></td><td align="left" valign="top"><p id="x_M61"><a name="x_M61"></a>Measure the performance of the length-based
	      approach, and compare with the depth approach.  Which
	      gives better performance results?</p></td></tr></tbody></table></div></div></div><div class="sect1" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both" id="id676390">Parallel strategies and MapReduce</h2></div></div></div><p id="x_N61"><a name="x_N61"></a>Within the programming community, one of the most famous
      software systems to credit functional programming for
      inspiration is Google's MapReduce infrastructure for parallel
      processing of bulk data.</p><p id="x_O61"><a name="x_O61"></a>We can easily construct a greatly simplified, but still
      useful, Haskell equivalent.  To focus our attention, we will
      look at the processing of web server log files, which tend to be
	both huge and plentiful<sup>[<a name="id676413" href="#ftn.id676413" class="footnote">57</a>]</sup>.</p><p id="x_P61"><a name="x_P61"></a>As an example, here is a log entry for a page visit recorded
      by the Apache web server.  The entry originally filled one line;
      we have split it across several lines to fit.</p><pre id="id676433" class="programlisting">201.49.94.87 - - [08/Jun/2008:07:04:20 -0500] "GET / HTTP/1.1"
200 2097 "http://en.wikipedia.org/wiki/Mercurial_(software)"
"Mozilla/5.0 (Windows; U; Windows XP 5.1; en-GB; rv:1.8.1.12)
Gecko/20080201 Firefox/2.0.0.12" 0 hgbook.red-bean.com</pre><p id="x_Q61"><a name="x_Q61"></a>While we could create a straightforward implementation
      without much effort, we will resist the temptation to dive in.
      If we think about solving a <span class="emphasis"><em>class</em></span> of
      problems instead of a single one, we may end up with more widely
      applicable code.</p><p id="x_R61"><a name="x_R61"></a>When we develop a parallel program, we are always faced with
      a few “<span class="quote">bad penny</span>” problems, which turn up no matter
      what the underlying programming language is.</p><div class="itemizedlist"><ul type="disc"><li><p id="x_S61"><a name="x_S61"></a>Our algorithm quickly becomes obscured by the details of
	  partitioning and communication.  This makes it difficult to
	  understand code, which in turn makes modifying it
	  risky.</p></li><li><p id="x_T61"><a name="x_T61"></a>Choosing a “<span class="quote">grain size</span>”—the smallest
	  unit of work parceled out to a core— can be
	  difficult. If the grain size is too small, cores spend so
	  much of their time on book-keeping that a parallel program
	  can easily become slower than a serial counterpart.  If the
	  grain size is too large, some cores may lie idle due to poor
	  load balancing.</p></li></ul></div><div class="sect2" lang="en"><div class="titlepage"><div><div><h3 class="title" id="concurrent.strategies">Separating algorithm from evaluation</h3></div></div></div><p id="x_U61"><a name="x_U61"></a>In parallel Haskell code, the clutter that would arise
	from communication code in a traditional language is replaced
	with the clutter of <code class="function">par</code> and
	<code class="function">pseq</code> annotations.  As an example, this
	function operates similarly to <code class="function">map</code>, but
	evaluates each element to weak head normal form (WHNF) in
	parallel as it goes.</p><a name="ParMap.hs:parallelMap"></a><pre id="ParMap.hs:parallelMap" class="programlisting">-- file: ch24/ParMap.hs
import Control.Parallel (par)

parallelMap :: (a -&gt; b) -&gt; [a] -&gt; [b]
parallelMap f (x:xs) = let r = f x
                       in r `par` r : parallelMap f xs
parallelMap _ _      = []</pre><p id="x_V61"><a name="x_V61"></a>The type <code class="varname">b</code> might be a list,
	or some other type for which evaluation to WHNF doesn't do a
	useful amount of work.  We'd prefer not to have to write a
	special <code class="function">parallelMap</code> for lists, and for
	every other type that needs special handling.</p><p id="x_W61"><a name="x_W61"></a>To address this problem, we will begin by considering a
	simpler problem: how to force a value to be evaluated.  Here
	is a function that forces every element of a list to be
	evaluated to WHNF.</p><a name="ParMap.hs:forceList"></a><pre id="ParMap.hs:forceList" class="programlisting">-- file: ch24/ParMap.hs
forceList :: [a] -&gt; ()
forceList (x:xs) = x `pseq` forceList xs
forceList _      = ()</pre><p id="x_X61"><a name="x_X61"></a>Our function performs no computation on the list. (In
	fact, from examining its type signature, we can tell that it
	<span class="emphasis"><em>cannot</em></span> perform any computation, since it
	knows nothing about the elements of the list.)  Its only
	purpose is to ensure that the spine of the list is evaluated
	to head normal form. The only place that it makes any sense to
	apply this function is in the first argument of
	<code class="function">seq</code> or <code class="function">par</code>, for
	example as follows.</p><a name="ParMap.hs:stricterMap"></a><pre id="ParMap.hs:stricterMap" class="programlisting">-- file: ch24/ParMap.hs
stricterMap :: (a -&gt; b) -&gt; [a] -&gt; [b]
stricterMap f xs = forceList xs `seq` map f xs</pre><p id="x_Y61"><a name="x_Y61"></a>This still leaves us with the elements of the list
	evaluated only to WHNF.  We address this by adding a function
	as parameter that can force an element to be evaluated more
	deeply.</p><a name="ParMap.hs:forceListAndElts"></a><pre id="ParMap.hs:forceListAndElts" class="programlisting">-- file: ch24/ParMap.hs
forceListAndElts :: (a -&gt; ()) -&gt; [a] -&gt; ()
forceListAndElts forceElt (x:xs) =
    forceElt x `seq` forceListAndElts forceElt xs
forceListAndElts _        _      = ()</pre><p id="x_Z61"><a name="x_Z61"></a>The <code class="code">Control.Parallel.Strategies</code>
	  module generalizes this idea into something we can use as a
	  library.  It introduces the idea of an <span class="emphasis"><em>evaluation
	    strategy</em></span>.</p><a name="Strat.hs:Strategy"></a><pre id="Strat.hs:Strategy" class="programlisting">-- file: ch24/Strat.hs
type Done = ()

type Strategy a = a -&gt; Done</pre><p id="x_a61"><a name="x_a61"></a>An evaluation strategy performs no computation; it simply
	ensures that a value is evaluated to some extent.  The
	simplest strategy is named <code class="function">r0</code>, and does
	nothing at all.</p><a name="Strat.hs:r0"></a><pre id="Strat.hs:r0" class="programlisting">-- file: ch24/Strat.hs
r0 :: Strategy a 
r0 _ = ()</pre><p id="x_b61"><a name="x_b61"></a>Next is <code class="function">rwhnf</code>, which evaluates a
	value to weak head normal form.</p><a name="Strat.hs:rwhnf"></a><pre id="Strat.hs:rwhnf" class="programlisting">-- file: ch24/Strat.hs
rwhnf :: Strategy a 
rwhnf x = x `seq` ()</pre><p id="x_c61"><a name="x_c61"></a>To evaluate a value to normal form, the module provides a
	typeclass with a method named <code class="function">rnf</code>.</p><a name="Strat.hs:NFData"></a><pre id="Strat.hs:NFData" class="programlisting">-- file: ch24/Strat.hs
class NFData a where
  rnf :: Strategy a
  rnf = rwhnf</pre><div class="tip"><table border="0" summary="Tip: Remembering those names"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Tip]" src="/support/figs/tip.png"></td><th align="left">Remembering those names</th></tr><tr><td align="left" valign="top"><p id="x_d61"><a name="x_d61"></a>If the names of these functions and types are not
	  sticking in your head, look at them as acronyms.  The name
	  <code class="function">rwhnf</code> expands to “<span class="quote">reduce to weak
	    head normal form</span>”; <span class="type">NFData</span> becomes
	  “<span class="quote">normal form data</span>”; and so on.</p></td></tr></table></div><p id="x_e61"><a name="x_e61"></a>For the basic types, such as <span class="type">Int</span>, weak head
	normal form and normal form are the same thing, which is why
	the <span class="type">NFData</span> typeclass uses
	<code class="function">rwhnf</code> as the default implementation of
	<code class="function">rnf</code>. For many common types, the
	<code class="code">Control.Parallel.Strategies</code> module provides
	instances of <span class="type">NFData</span>.</p><a name="Strat.hs:instances"></a><pre id="Strat.hs:instances" class="programlisting">-- file: ch24/Strat.hs
instance NFData Char
instance NFData Int

instance NFData a =&gt; NFData (Maybe a) where
    rnf Nothing  = ()
    rnf (Just x) = rnf x

{- ... and so on ... -}</pre><p id="x_f61"><a name="x_f61"></a>From these examples, it should be clear how you might
	write an <span class="type">NFData</span> instance for a type of your own.
	Your implementation of <code class="function">rnf</code> must handle
	every constructor, and apply <code class="function">rnf</code> to every
	field of a constructor.</p></div><div class="sect2" lang="en"><div class="titlepage"><div><div><h3 class="title" id="id676854">Separating algorithm from strategy</h3></div></div></div><p id="x_g61"><a name="x_g61"></a>From these strategy building blocks, we can construct more
	elaborate strategies.  Many are already provided by
	<code class="code">Control.Parallel.Strategies</code>.  For instance,
	<code class="function">parList</code> applies an evaluation strategy in
	parallel to every element of a list.</p><a name="Strat.hs:parList"></a><pre id="Strat.hs:parList" class="programlisting">-- file: ch24/Strat.hs
parList :: Strategy a -&gt; Strategy [a]
parList strat []     = ()
parList strat (x:xs) = strat x `par` (parList strat xs)</pre><p id="x_h61"><a name="x_h61"></a>The module uses this to define a parallel
	<code class="function">map</code> function.</p><a name="Strat.hs:parMap"></a><pre id="Strat.hs:parMap" class="programlisting">-- file: ch24/Strat.hs
parMap :: Strategy b -&gt; (a -&gt; b) -&gt; [a] -&gt; [b]
parMap strat f xs = map f xs `using` parList strat</pre><p id="x_i61"><a name="x_i61"></a>This is where the code becomes interesting.  On the left
	of <code class="function">using</code>, we have a normal application of
	<code class="function">map</code>.  On the right, we have an evaluation
	strategy.  The <code class="function">using</code> combinator tells us
	how to apply a strategy to a value, allowing us to keep the
	code separate from how we plan to evaluate it.</p><a name="Strat.hs:using"></a><pre id="Strat.hs:using" class="programlisting">-- file: ch24/Strat.hs
using :: a -&gt; Strategy a -&gt; a
using x s = s x `seq` x</pre><p id="x_j61"><a name="x_j61"></a>The <code class="code">Control.Parallel.Strategies</code> module
	provides many other functions that provide fine control over
	evaluation.  For instance, <code class="function">parZipWith</code>
	that applies <code class="function">zipWith</code> in parallel, using
	an evaluation strategy.</p><a name="Strat.hs:vectorSum"></a><pre id="Strat.hs:vectorSum" class="programlisting">-- file: ch24/Strat.hs
vectorSum' :: (NFData a, Num a) =&gt; [a] -&gt; [a] -&gt; [a]
vectorSum' = parZipWith rnf (+)</pre></div><div class="sect2" lang="en"><div class="titlepage"><div><div><h3 class="title" id="id676990">Writing a simple MapReduce definition</h3></div></div></div><p id="x_k61"><a name="x_k61"></a>We can quickly suggest a type for a
	<code class="function">mapReduce</code> function by considering what it
	must do.  We need a <span class="emphasis"><em>map</em></span> component, to
	which we will give the usual type <span class="type">a -&gt; b</span>.  And
	we need a <span class="emphasis"><em>reduce</em></span>; this term is a synonym
	for <span class="emphasis"><em>fold</em></span>.  Rather than commit ourselves
	to using a specific kind of fold, we'll use a more general
	type, <span class="type">[b] -&gt; c</span>.  This type lets us use a left
	or right fold, so we can choose the one that suits our data
	and processing needs.</p><p id="x_l61"><a name="x_l61"></a>If we plug these types together, the complete type looks
	like this.</p><a name="MapReduce.hs:simpleMapReduce.type"></a><pre id="MapReduce.hs:simpleMapReduce.type" class="programlisting">-- file: ch24/MapReduce.hs
simpleMapReduce
    :: (a -&gt; b)      -- map function
    -&gt; ([b] -&gt; c)    -- reduce function
    -&gt; [a]           -- list to map over
    -&gt; c</pre><p id="x_m61"><a name="x_m61"></a>The code that goes with the type is extremely
	simple.</p><a name="MapReduce.hs:simpleMapReduce"></a><pre id="MapReduce.hs:simpleMapReduce" class="programlisting">-- file: ch24/MapReduce.hs
simpleMapReduce mapFunc reduceFunc = reduceFunc . map mapFunc</pre></div><div class="sect2" lang="en"><div class="titlepage"><div><div><h3 class="title" id="id677069">MapReduce and strategies</h3></div></div></div><p id="x_n61"><a name="x_n61"></a>Our definition of <code class="function">simpleMapReduce</code> is
	too simple to really be interesting.  To make it useful, we
	want to be able to specify that some of the work should occur
	in parallel.  We'll achieve this using strategies, passing in
	a strategy for the map phase and one for the reduction phase.</p><a name="MapReduce.hs:mapReduce.type"></a><pre id="MapReduce.hs:mapReduce.type" class="programlisting">-- file: ch24/MapReduce.hs
mapReduce
    :: Strategy b    -- evaluation strategy for mapping
    -&gt; (a -&gt; b)      -- map function
    -&gt; Strategy c    -- evaluation strategy for reduction
    -&gt; ([b] -&gt; c)    -- reduce function
    -&gt; [a]           -- list to map over
    -&gt; c</pre><p id="x_o61"><a name="x_o61"></a>Both the type and the body of the function must grow a
	little in size to accommodate the strategy parameters.</p><a name="MapReduce.hs:mapReduce"></a><pre id="MapReduce.hs:mapReduce" class="programlisting">-- file: ch24/MapReduce.hs
mapReduce mapStrat mapFunc reduceStrat reduceFunc input =
    mapResult `pseq` reduceResult
  where mapResult    = parMap mapStrat mapFunc input
        reduceResult = reduceFunc mapResult `using` reduceStrat</pre></div><div class="sect2" lang="en"><div class="titlepage"><div><div><h3 class="title" id="id677123">Sizing work appropriately</h3></div></div></div><p id="x_p61"><a name="x_p61"></a>To achieve decent performance, we must ensure that the
	work that we do per application of <code class="function">par</code>
	substantially outweighs its book-keeping costs.  If we are
	processing a huge file, splitting it on line boundaries gives
	us far too little work compared to overhead.</p><p id="x_q61"><a name="x_q61"></a>We will develop a way to process a file in larger chunks
	in a later section.  What should those chunks consist of?
	Because a web server log file ought to contain only ASCII
	text, we will see excellent performance with a lazy
	<span class="type">ByteString</span>: this type is highly efficient, and
	consumes little memory when we stream it from a file.</p><a name="LineChunks.hs:withChunks"></a><pre id="LineChunks.hs:withChunks" class="programlisting">-- file: ch24/LineChunks.hs
module LineChunks
    (
      chunkedReadWith
    ) where

import Control.Exception (bracket, finally)
import Control.Monad (forM, liftM)
import Control.Parallel.Strategies (NFData, rnf)
import Data.Int (Int64)
import qualified Data.ByteString.Lazy.Char8 as LB
import GHC.Conc (numCapabilities)
import System.IO

data ChunkSpec = CS {
      chunkOffset :: !Int64
    , chunkLength :: !Int64
    } deriving (Eq, Show)

withChunks :: (NFData a) =&gt;
              (FilePath -&gt; IO [ChunkSpec])
           -&gt; ([LB.ByteString] -&gt; a)
           -&gt; FilePath
           -&gt; IO a
withChunks chunkFunc process path = do
  (chunks, handles) &lt;- chunkedRead chunkFunc path
  let r = process chunks
  (rnf r `seq` return r) `finally` mapM_ hClose handles
  
chunkedReadWith :: (NFData a) =&gt;
                   ([LB.ByteString] -&gt; a) -&gt; FilePath -&gt; IO a
chunkedReadWith func path =
    withChunks (lineChunks (numCapabilities * 4)) func path</pre><p id="x_r61"><a name="x_r61"></a>We consume each chunk in parallel, taking careful
	advantage of lazy I/O to ensure that we can stream these
	chunks safely.</p><div class="sect3" lang="en"><div class="titlepage"><div><div><h4 class="title" id="id677193">Mitigating the risks of lazy I/O</h4></div></div></div><p id="x_s61"><a name="x_s61"></a>Lazy I/O poses a few well known hazards that
	    we would like to avoid.</p><div class="itemizedlist"><ul type="disc"><li><p id="x_t61"><a name="x_t61"></a>We may invisibly keep a file handle open for longer
	      than necessary, by not forcing the computation that
	      pulls data from it to be evaluated.  Since an operating
	      system will typically place a small, fixed limit on the
	      number of files we can have open at once, if we do not
	      address this risk, we can accidentally starve some other
	      part of our program of file handles.</p></li><li><p id="x_u61"><a name="x_u61"></a>If we do not explicitly close a file handle, the
	      garbage collector will automatically close it for us. It
	      may take a long time to notice that it should close the
	      file handle.  This poses the same starvation risk as
	      above.</p></li><li><p id="x_v61"><a name="x_v61"></a>We can avoid starvation by explicitly closing a file
	      handle. If we do so too early, though, we can cause a
	      lazy computation to fail if it expects to be able to
	      pull more data from a closed file handle.</p></li></ul></div><p id="x_w61"><a name="x_w61"></a>On top of these well-known risks, we cannot use a single
	  file handle to supply data to multiple threads.  A file
	  handle has a single “<span class="quote">seek pointer</span>” that tracks
	  the position from which it should be reading, but when we
	  want to read multiple chunks, each needs to consume data
	  from a different position in the file.</p><p id="x_x61"><a name="x_x61"></a>With these ideas in mind, let's fill out the lazy I/O
	  picture.</p><a name="LineChunks.hs:chunkedRead"></a><pre id="LineChunks.hs:chunkedRead" class="programlisting">-- file: ch24/LineChunks.hs
chunkedRead :: (FilePath -&gt; IO [ChunkSpec])
            -&gt; FilePath
            -&gt; IO ([LB.ByteString], [Handle])
chunkedRead chunkFunc path = do
  chunks &lt;- chunkFunc path
  liftM unzip . forM chunks $ \spec -&gt; do
    h &lt;- openFile path ReadMode
    hSeek h AbsoluteSeek (fromIntegral (chunkOffset spec))
    chunk &lt;- LB.take (chunkLength spec) `liftM` LB.hGetContents h
    return (chunk, h)</pre><p id="x_y61"><a name="x_y61"></a>We avoid the starvation problem by explicitly closing
	  file handles.  We allow multiple threads to read different
	  chunks at once by supplying each one with a distinct file
	  handle, all reading the same file.</p><p id="x_z61"><a name="x_z61"></a>The final problem that we try to mitigate is that of a
	  lazy computation having a file handle closed behind its
	  back.  We use <code class="function">rnf</code> to force all of our
	  processing to complete before we return from
	  <code class="function">withChunks</code>. We can then close our file
	  handles explicitly, as they should no longer be read from.
	  If you must use lazy I/O in a program, it is often best to
	  “<span class="quote">firewall</span>” it like this, so that it cannot
	  cause problems in unexpected parts of your code.</p><div class="tip"><table border="0" summary="Tip: Processing chunks via a fold"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Tip]" src="/support/figs/tip.png"></td><th align="left">Processing chunks via a fold</th></tr><tr><td align="left" valign="top"><p id="x_A71"><a name="x_A71"></a>We can adapt the fold-with-early-termination technique
	    from <a class="xref" href="io-case-study-a-library-for-searching-the-filesystem.html#find.fold" title="Another way of looking at traversal">the section called “Another way of looking at traversal”</a> to stream-based file
	    processing. While this requires more work than the lazy
	    I/O approach, it nicely avoids the above problems.</p></td></tr></table></div></div></div><div class="sect2" lang="en"><div class="titlepage"><div><div><h3 class="title" id="id677345">Efficiently finding line-aligned chunks</h3></div></div></div><p id="x_B71"><a name="x_B71"></a>Since a server log file is line-oriented, we need an
	efficient way to break a file into large chunks, while making
	sure that each chunk ends on a line boundary.  Since a chunk
	might be tens of megabytes in size, we don't want to scan all
	of the data in a chunk to determine where its final boundary
	should be.</p><p id="x_C71"><a name="x_C71"></a>Our approach works whether we choose a fixed chunk size or
	a fixed number of chunks.  Here, we opt for the latter.  We
	begin by seeking to the approximate position of the end of a
	chunk, then scan forwards until we reach a newline character.
	We then start the next chunk after the newline, and repeat the
	procedure.</p><a name="LineChunks.hs:lineChunks"></a><pre id="LineChunks.hs:lineChunks" class="programlisting">-- file: ch24/LineChunks.hs
lineChunks :: Int -&gt; FilePath -&gt; IO [ChunkSpec]
lineChunks numChunks path = do
  bracket (openFile path ReadMode) hClose $ \h -&gt; do
    totalSize &lt;- fromIntegral `liftM` hFileSize h
    let chunkSize = totalSize `div` fromIntegral numChunks
        findChunks offset = do
          let newOffset = offset + chunkSize
          hSeek h AbsoluteSeek (fromIntegral newOffset)
          let findNewline off = do
                eof &lt;- hIsEOF h
                if eof
                  then return [CS offset (totalSize - offset)]
                  else do
                    bytes &lt;- LB.hGet h 4096
                    case LB.elemIndex '\n' bytes of
                      Just n -&gt; do
                        chunks@(c:_) &lt;- findChunks (off + n + 1)
                        let coff = chunkOffset c
                        return (CS offset (coff - offset):chunks)
                      Nothing -&gt; findNewline (off + LB.length bytes)
          findNewline newOffset
    findChunks 0</pre><p id="x_D71"><a name="x_D71"></a>The last chunk will end up a little shorter than its
	predecessors, but this difference will be insignificant in
	practice.</p></div><div class="sect2" lang="en"><div class="titlepage"><div><div><h3 class="title" id="id677400">Counting lines</h3></div></div></div><p id="x_E71"><a name="x_E71"></a>This simple example illustrates how to use the scaffolding
	we have built.</p><a name="LineCount.hs:countLines"></a><pre id="LineCount.hs:countLines" class="programlisting">-- file: ch24/LineCount.hs
module Main where

import Control.Monad (forM_)
import Data.Int (Int64)
import qualified Data.ByteString.Lazy.Char8 as LB
import System.Environment (getArgs)

import LineChunks (chunkedReadWith)
import MapReduce (mapReduce, rnf)

lineCount :: [LB.ByteString] -&gt; Int64
lineCount = mapReduce rnf (LB.count '\n')
                      rnf sum

main :: IO ()
main = do
  args &lt;- getArgs
  forM_ args $ \path -&gt; do
    numLines &lt;- chunkedReadWith lineCount path
    putStrLn $ path ++ ": " ++ show numLines</pre><p id="x_F71"><a name="x_F71"></a>If we compile this program with <code class="option">ghc -O2 --make
	  -threaded</code>, it should perform well after an initial
	run to “<span class="quote">warm</span>” the filesystem cache. On a dual
	core laptop, processing a log file 248 megabytes (1.1 million
	lines) in size, this program runs in 0.576 seconds using a
	single core, and 0.361 with two (using <code class="option">+RTS
	  -N2</code>).</p></div><div class="sect2" lang="en"><div class="titlepage"><div><div><h3 class="title" id="id677449">Finding the most popular URLs</h3></div></div></div><p id="x_G71"><a name="x_G71"></a>In this example, we count the number of times each URL is
	accessed.  This example comes from [<a href="bibliography.html#bib.google08" class="biblioref" title="[Google08]"><abbr class="abbrev">Google08</abbr></a>], Google's original paper discussing
	MapReduce.  In the <span class="emphasis"><em>map</em></span> phase, for each
	chunk, we create a <span class="type">Map</span> from URL to the number of
	times it was accessed.  In the <span class="emphasis"><em>reduce</em></span>
	phase, we union-merge these maps into one.</p><a name="CommonURLs.hs:countURLs"></a><pre id="CommonURLs.hs:countURLs" class="programlisting">-- file: ch24/CommonURLs.hs
module Main where

import Control.Parallel.Strategies (NFData(..), rwhnf)
import Control.Monad (forM_)
import Data.List (foldl', sortBy)
import qualified Data.ByteString.Lazy.Char8 as L
import qualified Data.ByteString.Char8 as S
import qualified Data.Map as M
import Text.Regex.PCRE.Light (compile, match)

import System.Environment (getArgs)
import LineChunks (chunkedReadWith)
import MapReduce (mapReduce)

countURLs :: [L.ByteString] -&gt; M.Map S.ByteString Int
countURLs = mapReduce rwhnf (foldl' augment M.empty . L.lines)
                      rwhnf (M.unionsWith (+))
  where augment map line =
            case match (compile pattern []) (strict line) [] of
              Just (_:url:_) -&gt; M.insertWith' (+) url 1 map
              _ -&gt; map
        strict  = S.concat . L.toChunks
        pattern = S.pack "\"(?:GET|POST|HEAD) ([^ ]+) HTTP/"</pre><p id="x_H71"><a name="x_H71"></a>To pick a URL out of a line of the log file, we use the
	bindings to the PCRE regular expression library that we
	developed in <a class="xref" href="interfacing-with-c-the-ffi.html" title="Chapter 17. Interfacing with C: the FFI">Chapter 17, <i>Interfacing with C: the FFI</i></a>.</p><p id="x_I71"><a name="x_I71"></a>Our driver function prints the ten most popular URLs.  As
	with the line counting example, this program runs about 1.8
	times faster with two cores than with one, taking 1.7 seconds
	to process the a log file containing 1.1 million
	entries.</p></div><div class="sect2" lang="en"><div class="titlepage"><div><div><h3 class="title" id="id677523">Conclusions</h3></div></div></div><p id="x_J71"><a name="x_J71"></a>Given a problem that fits its model well, the MapReduce
	programming model lets us write “<span class="quote">casual</span>” parallel
	programs in Haskell with good performance, and  minimal
	additional effort.  We can easily extend the idea to use other
	data sources, such as collections of files, or data sourced
	over the network.</p><p id="x_K71"><a name="x_K71"></a>In many cases, the performance bottleneck will be
	streaming data at a rate high enough to keep up with a core's
	processing capacity. For instance, if we try to use either of
	the above sample programs on a file that is not cached in
	memory or streamed from a high-bandwidth storage array, we
	will spend most of our time waiting for disk I/O, gaining no
	benefit from multiple cores.</p></div></div><div class="footnotes"><br><hr width="100" align="left"><div class="footnote"><p><sup>[<a name="ftn.id673063" href="#id673063" class="para">54</a>] </sup>As we will show later, <span class="application">GHC</span> threads are extraordinarily
	  lightweight.  If the runtime were to provide a way to check
	  the status of every thread, the overhead of every thread
	  would increase, even if this information were never
	  used.</p></div><div class="footnote"><p><sup>[<a name="ftn.id674894" href="#id674894" class="para">55</a>] </sup>The non-threaded runtime does not understand this
	    option, and will reject it with an error message.</p></div><div class="footnote"><p><sup>[<a name="ftn.id675039" href="#id675039" class="para">56</a>] </sup>As we write this book, the garbage collector is being
	    retooled to use multiple cores, but we cannot yet predict
	    its future effect.</p></div><div class="footnote"><p><sup>[<a name="ftn.id676413" href="#id676413" class="para">57</a>] </sup>The genesis of this idea comes from Tim Bray.</p></div></div></div><div class="rwhfooter"><p><img src="/support/figs/rss.png"> Want to stay up to date? Subscribe to the comment feed for <a id="chapterfeed" class="feed" href="/feeds/comments/">this chapter</a>, or the <a class="feed" href="/feeds/comments/">entire book</a>.</p><p>Copyright 2007, 2008 Bryan O'Sullivan, Don Stewart, and
      John Goerzen. This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc/3.0/">Creative
      Commons Attribution-Noncommercial 3.0 License</a>. Icons by <a href="mailto:mattahan@gmail.com">Paul Davey</a> aka <a href="http://mattahan.deviantart.com/">Mattahan</a>.</p></div><div class="navfooter"><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="gui-programming-with-gtk-hs.html">Prev</a> </td><td width="20%" align="center"> </td><td width="40%" align="right"> <a accesskey="n" href="profiling-and-optimization.html">Next</a></td></tr><tr><td width="40%" align="left" valign="top">Chapter 23. GUI Programming with gtk2hs </td><td width="20%" align="center"><a accesskey="h" href="index.html">Home</a></td><td width="40%" align="right" valign="top"> Chapter 25. Profiling and optimization</td></tr></table></div><script src="http://www.google-analytics.com/urchin.js" type="text/javascript"></script><script type="text/javascript">_uacct = "UA-1805907-3"; urchinTracker();</script></body></html>
